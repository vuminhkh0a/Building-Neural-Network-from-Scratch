{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9e346f",
   "metadata": {},
   "source": [
    "# Neuron networks from scratch in Python\n",
    "References: http://103.203.175.90:81/fdScript/RootOfEBooks/E%20Book%20collection%20-%202024%20-%20G/CSE%20%20IT%20AIDS%20ML/Neural%20Network.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0666c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd975832",
   "metadata": {},
   "source": [
    "## Chapter 19: A real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a9f3f",
   "metadata": {},
   "source": [
    "### 19.1 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "802044f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a407cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "833ee63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping images\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(FILE):\n",
    "    print(f'Downloading {URL} and saving as {FILE}')\n",
    "    urllib.request.urlretrieve(URL, FILE)\n",
    "\n",
    "print('Unzipping images')\n",
    "with ZipFile(FILE) as zip_images:\n",
    "    zip_images.extractall(FOLDER)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b8aa1",
   "metadata": {},
   "source": [
    "### 19.2 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b13efcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   9   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   6  30   3   4   4  13  21  30  49  31   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 158 229 214 215 208 212 215 214 227 228   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 202 206 205 212 233 244 214 200 205 196   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 226 208 196 182 240 211 202 194 194 252   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 232 204 203 176 217 208 205 192 184 251  31   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   8 253 202 199 180 205 196 190 202 192 233  57   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  26 250 185 197 180 199 196 182 203 187 209  72   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  43 241 190 197 184 199 198 181 204 179 198  92   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  44 241 197 193 184 206 210 187 206 176 202  98   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  50 239 194 181 188 190 151 204 192 175 200  95   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  62 233 194 178 193 186 153 215 188 170 197  98   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  70 228 192 175 196 173 127 215 194 170 194 107   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  76 228 192 170 197 167 111 205 194 170 194 114   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  79 235 192 169 203 162 104 210 196 167 199 121   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  82 238 193 167 211 145  80 216 192 168 199 126   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  82 233 192 165 223 119  43 221 190 179 199 129   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  89 232 187 159 227  93  22 233 185 176 194 134   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  96 233 188 162 242  80   0 238 186 175 198 144   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  97 238 193 162 254  48   0 224 194 176 200 147   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  97 241 196 169 250  14   0 206 205 178 200 147   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  91 244 190 182 246   3   0 190 202 181 204 145   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  82 246 200 188 233   0   0 165 204 192 206 140   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  82 212 204 198 199   0   0 138 208 191 202 132   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 115 232 214 220 163   0   0 109 211 199 215 164   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  76 199 175 209 116   0   0  63 194 179 202  95   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   6   0   0   0   0   8   4   0   0   0   3   7   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image_data = cv2.imread('fashion_mnist_images/train/1/0001.png', cv2.IMREAD_UNCHANGED)\n",
    "np.set_printoptions(linewidth=200)\n",
    "print(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "43ab09e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_data)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fff9477c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_data, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "aeb5868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset(dataset, path):\n",
    "    labels = sorted(os.listdir('fashion_mnist_images/train'))\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "def create_data_mnist(path):\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a4e3bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245908a",
   "metadata": {},
   "source": [
    "### 19.3 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dd725fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 1.0\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "print(X.min(), X.max())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1b06ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e1253",
   "metadata": {},
   "source": [
    "### 19.4 Data shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "52e9cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[2 2 2 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y[0:6000])\n",
    "print(y[6000:12000])\n",
    "print(y[12001:18000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9d5628c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 59997 59998 59999]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = np.array(range(X.shape[0]))\n",
    "print(keys)\n",
    "plt.axis('off')\n",
    "plt.imshow(X[3].reshape((28, 28)))\n",
    "\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2b3c9aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3048 19563 58303 ... 42613 43567  2732]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7dedd4233150>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(keys)\n",
    "plt.axis('off')\n",
    "plt.imshow(X[3].reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b0a04",
   "metadata": {},
   "source": [
    "### 19.5 Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "87eb1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "steps = X.shape[0] // BATCH_SIZE\n",
    "\n",
    "if steps * BATCH_SIZE < X.shape[0]:\n",
    "    steps += 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step in range(steps):\n",
    "        batch_X = X[step*BATCH_SIZE:(step+1)*BATCH_SIZE]\n",
    "        batch_y = y[step*BATCH_SIZE:(step+1)*BATCH_SIZE]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5fcbcf",
   "metadata": {},
   "source": [
    "### Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "11412157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, l1_weight_regularizer=0., l1_bias_regularizer=0.,\n",
    "                 l2_weight_regularizer=0., l2_bias_regularizer=0.):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        self.l1_weight_regularizer = l1_weight_regularizer\n",
    "        self.l1_bias_regularizer = l1_bias_regularizer\n",
    "        self.l2_weight_regularizer = l2_weight_regularizer\n",
    "        self.l2_bias_regularizer = l2_bias_regularizer\n",
    "    \n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "        if self.l1_weight_regularizer > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.l1_weight_regularizer * dL1\n",
    "\n",
    "        if self.l1_bias_regularizer > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.l1_bias_regularizer * dL1\n",
    "\n",
    "        if self.l2_weight_regularizer > 0:\n",
    "            self.dweights += 2 * self.l2_weight_regularizer * self.weights\n",
    "        \n",
    "        if self.l2_bias_regularizer > 0:\n",
    "            self.dbiases += 2 * self.l2_bias_regularizer * self.biases\n",
    "\n",
    "class Layer_Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.rate = 1 - dropout_rate\n",
    "    \n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, inputs.shape) / self.rate\n",
    "        \n",
    "        self.output = self.binary_mask * inputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "class Layer_Input:\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "\n",
    "# ACTIVATION\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "    \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        exp_val = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_val / np.sum(exp_val, axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        for index, (single_output, single_dvalue) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = np.reshape(single_output, (-1, 1))\n",
    "            jacobian = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian, single_dvalue)\n",
    "    \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1. / 1. + np.exp(-inputs)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "    \n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "class Activation_Linear:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "# LOSS\n",
    "\n",
    "class Loss:\n",
    "    def regularization_loss(self):\n",
    "        reg_loss = 0\n",
    "\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.l1_weight_regularizer > 0:\n",
    "                reg_loss += layer.l1_weight_regularizer * np.sum(np.abs(layer.weights))\n",
    "            if layer.l1_bias_regularizer > 0:\n",
    "                reg_loss += layer.l1_bias_regularizer * np.sum(np.abs(layer.biases))\n",
    "            if layer.l2_weight_regularizer > 0:\n",
    "                reg_loss += layer.l2_weight_regularizer * np.sum(layer.weights ** 2)\n",
    "            if layer.l2_bias_regularizer > 0:\n",
    "                reg_loss += layer.l2_bias_regularizer * np.sum(layer.biases ** 2)\n",
    "        \n",
    "        return reg_loss\n",
    "\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred[range(samples), y_true] \n",
    "        \n",
    "        if len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred * y_true, axis=1)\n",
    "        \n",
    "        return - np.log(correct_confidences)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        self.dinputs = - y_true / dvalues / samples\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs /= samples\n",
    "\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        sample_losses = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "        return np.mean(sample_losses, axis=-1)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        self.dinputs = - (y_true / dvalues - (1 - y_true) / (1 - dvalues)) / outputs / samples\n",
    "\n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return np.mean((y_true - y_pred)**2, axis=-1)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs / samples\n",
    "\n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs / samples\n",
    "\n",
    "# ACCURACY\n",
    "\n",
    "class Accuracy:\n",
    "    def calculate(self, predictions, y):\n",
    "        comparisions = self.compare(predictions, y)\n",
    "        accuracy = np.mean(comparisions)\n",
    "\n",
    "        self.accumulated_sum += np.sum(comparisions)\n",
    "        self.accumulated_count += len(comparisions)\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        return accuracy\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    def __init__(self, *, binary=False):\n",
    "        self.binary = binary\n",
    "    \n",
    "    def init(self, y):\n",
    "        pass\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        if not self.binary and len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y\n",
    "\n",
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "    \n",
    "# OPTIMIZER\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., eps=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate / (1. + self.decay * self.iterations)\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "            \n",
    "        layer.weights -= self.current_learning_rate * weight_momentums_corrected / \\\n",
    "        (np.sqrt(weight_cache_corrected) + self.eps)\n",
    "        layer.biases -= self.current_learning_rate * bias_momentums_corrected / \\\n",
    "        (np.sqrt(bias_cache_corrected) + self.eps)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d8ff8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "    \n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        layer_count = len(self.layers)\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "            \n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "        \n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "            isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data=None):\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        train_steps = 1\n",
    "\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "            X_val, y_val = validation_data\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            if validation_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "            \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            print(f'epoch: {epoch}')\n",
    "\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            for step in range(train_steps):\n",
    "\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                \n",
    "                else:\n",
    "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "                \n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                data_loss, regularization_loss = \\\n",
    "                self.loss.calculate(output, batch_y, include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, ' +\n",
    "                          f'acc: {accuracy:.3f}, ' +\n",
    "                          f'loss: {loss:.3f} (' +\n",
    "                          f'data_loss: {data_loss:.3f}, ' +\n",
    "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            epoch_data_loss, epoch_regularization_loss = \\\n",
    "                self.loss.calculate_accumulated(include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(f'training, ' +\n",
    "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                  f'loss: {epoch_loss:.3f} (' +\n",
    "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "            \n",
    "            if validation_data is not None:\n",
    "                self.loss.new_pass()\n",
    "                self.accuracy.new_pass()\n",
    "\n",
    "                for step in range(validation_steps):\n",
    "                    if batch_size is None:\n",
    "                        batch_X = X_val\n",
    "                        batch_y = y_val\n",
    "                    \n",
    "                    else:\n",
    "                        batch_X = X_val[step*batch_size:(step+1)*batch_size]\n",
    "                        batch_y = y_val[step*batch_size:(step+1)*batch_size]    \n",
    "\n",
    "                    output = self.forward(batch_X, training=False)\n",
    "                    self.loss.calculate(output, batch_y)\n",
    "                    predictions = self.output_layer_activation.predictions(output)\n",
    "                    self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                validation_loss = self.loss.calculate_accumulated()\n",
    "                validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "                print(f'validation, acc: {validation_accuracy:.3f}, loss: {validation_loss:.3f}')\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        return layer.output\n",
    "    \n",
    "    def backward(self, output, y):\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "            \n",
    "            return\n",
    "        \n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c8fe8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.062, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.773, loss: 0.722 (data_loss: 0.722, reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "step: 200, acc: 0.766, loss: 0.672 (data_loss: 0.672, reg_loss: 0.000), lr: 0.0008333333333333334\n",
      "step: 300, acc: 0.820, loss: 0.508 (data_loss: 0.508, reg_loss: 0.000), lr: 0.0007692307692307692\n",
      "step: 400, acc: 0.844, loss: 0.452 (data_loss: 0.452, reg_loss: 0.000), lr: 0.0007142857142857144\n",
      "step: 468, acc: 0.823, loss: 0.581 (data_loss: 0.581, reg_loss: 0.000), lr: 0.0006811989100817438\n",
      "training, acc: 0.747, loss: 0.677 (data_loss: 0.677, reg_loss: 0.000), lr: 0.0006811989100817438\n",
      "validation, acc: 0.818, loss: 0.494\n",
      "epoch: 2\n",
      "step: 0, acc: 0.836, loss: 0.484 (data_loss: 0.484, reg_loss: 0.000), lr: 0.0006807351940095302\n",
      "step: 100, acc: 0.852, loss: 0.372 (data_loss: 0.372, reg_loss: 0.000), lr: 0.0006373486297004462\n",
      "step: 200, acc: 0.820, loss: 0.498 (data_loss: 0.498, reg_loss: 0.000), lr: 0.0005991611743559018\n",
      "step: 300, acc: 0.883, loss: 0.394 (data_loss: 0.394, reg_loss: 0.000), lr: 0.0005652911249293386\n",
      "step: 400, acc: 0.859, loss: 0.382 (data_loss: 0.382, reg_loss: 0.000), lr: 0.0005350454788657035\n",
      "step: 468, acc: 0.865, loss: 0.524 (data_loss: 0.524, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.839, loss: 0.443 (data_loss: 0.443, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "validation, acc: 0.838, loss: 0.446\n",
      "epoch: 3\n",
      "step: 0, acc: 0.875, loss: 0.373 (data_loss: 0.373, reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "step: 100, acc: 0.852, loss: 0.345 (data_loss: 0.345, reg_loss: 0.000), lr: 0.0004906771344455348\n",
      "step: 200, acc: 0.812, loss: 0.444 (data_loss: 0.444, reg_loss: 0.000), lr: 0.0004677268475210477\n",
      "step: 300, acc: 0.883, loss: 0.342 (data_loss: 0.342, reg_loss: 0.000), lr: 0.00044682752457551384\n",
      "step: 400, acc: 0.867, loss: 0.365 (data_loss: 0.365, reg_loss: 0.000), lr: 0.000427715996578272\n",
      "step: 468, acc: 0.833, loss: 0.525 (data_loss: 0.525, reg_loss: 0.000), lr: 0.00041562759767248546\n",
      "training, acc: 0.856, loss: 0.396 (data_loss: 0.396, reg_loss: 0.000), lr: 0.00041562759767248546\n",
      "validation, acc: 0.850, loss: 0.415\n",
      "epoch: 4\n",
      "step: 0, acc: 0.906, loss: 0.333 (data_loss: 0.333, reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "step: 100, acc: 0.859, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000), lr: 0.00039888312724371757\n",
      "step: 200, acc: 0.867, loss: 0.391 (data_loss: 0.391, reg_loss: 0.000), lr: 0.0003835826620636747\n",
      "step: 300, acc: 0.883, loss: 0.328 (data_loss: 0.328, reg_loss: 0.000), lr: 0.00036941263391207984\n",
      "step: 400, acc: 0.859, loss: 0.346 (data_loss: 0.346, reg_loss: 0.000), lr: 0.0003562522265764161\n",
      "step: 468, acc: 0.854, loss: 0.470 (data_loss: 0.470, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.865, loss: 0.370 (data_loss: 0.370, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "validation, acc: 0.854, loss: 0.404\n",
      "epoch: 5\n",
      "step: 0, acc: 0.914, loss: 0.304 (data_loss: 0.304, reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "step: 100, acc: 0.875, loss: 0.294 (data_loss: 0.294, reg_loss: 0.000), lr: 0.0003360215053763441\n",
      "step: 200, acc: 0.844, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.00032509752925877764\n",
      "step: 300, acc: 0.891, loss: 0.335 (data_loss: 0.335, reg_loss: 0.000), lr: 0.00031486146095717883\n",
      "step: 400, acc: 0.852, loss: 0.336 (data_loss: 0.336, reg_loss: 0.000), lr: 0.00030525030525030525\n",
      "step: 468, acc: 0.875, loss: 0.458 (data_loss: 0.458, reg_loss: 0.000), lr: 0.000299043062200957\n",
      "training, acc: 0.871, loss: 0.354 (data_loss: 0.354, reg_loss: 0.000), lr: 0.000299043062200957\n",
      "validation, acc: 0.859, loss: 0.389\n",
      "epoch: 6\n",
      "step: 0, acc: 0.891, loss: 0.324 (data_loss: 0.324, reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "step: 100, acc: 0.875, loss: 0.253 (data_loss: 0.253, reg_loss: 0.000), lr: 0.0002902757619738752\n",
      "step: 200, acc: 0.852, loss: 0.363 (data_loss: 0.363, reg_loss: 0.000), lr: 0.0002820874471086037\n",
      "step: 300, acc: 0.922, loss: 0.285 (data_loss: 0.285, reg_loss: 0.000), lr: 0.00027434842249657066\n",
      "step: 400, acc: 0.859, loss: 0.341 (data_loss: 0.341, reg_loss: 0.000), lr: 0.000267022696929239\n",
      "step: 468, acc: 0.854, loss: 0.438 (data_loss: 0.438, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.875, loss: 0.340 (data_loss: 0.340, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "validation, acc: 0.862, loss: 0.383\n",
      "epoch: 7\n",
      "step: 0, acc: 0.906, loss: 0.285 (data_loss: 0.285, reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "step: 100, acc: 0.914, loss: 0.235 (data_loss: 0.235, reg_loss: 0.000), lr: 0.00025549310168625444\n",
      "step: 200, acc: 0.867, loss: 0.355 (data_loss: 0.355, reg_loss: 0.000), lr: 0.00024912805181863477\n",
      "step: 300, acc: 0.906, loss: 0.303 (data_loss: 0.303, reg_loss: 0.000), lr: 0.0002430724355858046\n",
      "step: 400, acc: 0.836, loss: 0.345 (data_loss: 0.345, reg_loss: 0.000), lr: 0.00023730422401518745\n",
      "step: 468, acc: 0.875, loss: 0.404 (data_loss: 0.404, reg_loss: 0.000), lr: 0.00023353573096683794\n",
      "training, acc: 0.879, loss: 0.329 (data_loss: 0.329, reg_loss: 0.000), lr: 0.00023353573096683794\n",
      "validation, acc: 0.866, loss: 0.377\n",
      "epoch: 8\n",
      "step: 0, acc: 0.898, loss: 0.313 (data_loss: 0.313, reg_loss: 0.000), lr: 0.0002334812047630166\n",
      "step: 100, acc: 0.906, loss: 0.210 (data_loss: 0.210, reg_loss: 0.000), lr: 0.00022815423226100844\n",
      "step: 200, acc: 0.875, loss: 0.353 (data_loss: 0.353, reg_loss: 0.000), lr: 0.0002230649118893598\n",
      "step: 300, acc: 0.906, loss: 0.289 (data_loss: 0.289, reg_loss: 0.000), lr: 0.0002181976871045167\n",
      "step: 400, acc: 0.875, loss: 0.335 (data_loss: 0.335, reg_loss: 0.000), lr: 0.0002135383301302584\n",
      "step: 468, acc: 0.875, loss: 0.394 (data_loss: 0.394, reg_loss: 0.000), lr: 0.0002104820037886761\n",
      "training, acc: 0.883, loss: 0.319 (data_loss: 0.319, reg_loss: 0.000), lr: 0.0002104820037886761\n",
      "validation, acc: 0.865, loss: 0.371\n",
      "epoch: 9\n",
      "step: 0, acc: 0.906, loss: 0.302 (data_loss: 0.302, reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "step: 100, acc: 0.891, loss: 0.229 (data_loss: 0.229, reg_loss: 0.000), lr: 0.00020610057708161583\n",
      "step: 200, acc: 0.867, loss: 0.327 (data_loss: 0.327, reg_loss: 0.000), lr: 0.00020193861066235866\n",
      "step: 300, acc: 0.898, loss: 0.251 (data_loss: 0.251, reg_loss: 0.000), lr: 0.0001979414093428345\n",
      "step: 400, acc: 0.883, loss: 0.323 (data_loss: 0.323, reg_loss: 0.000), lr: 0.00019409937888198756\n",
      "step: 468, acc: 0.875, loss: 0.374 (data_loss: 0.374, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.885, loss: 0.313 (data_loss: 0.313, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "validation, acc: 0.867, loss: 0.370\n",
      "epoch: 10\n",
      "step: 0, acc: 0.891, loss: 0.269 (data_loss: 0.269, reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "step: 100, acc: 0.898, loss: 0.231 (data_loss: 0.231, reg_loss: 0.000), lr: 0.00018793459875963167\n",
      "step: 200, acc: 0.875, loss: 0.328 (data_loss: 0.328, reg_loss: 0.000), lr: 0.00018446781036709093\n",
      "step: 300, acc: 0.922, loss: 0.258 (data_loss: 0.258, reg_loss: 0.000), lr: 0.00018112660749864155\n",
      "step: 400, acc: 0.867, loss: 0.302 (data_loss: 0.302, reg_loss: 0.000), lr: 0.0001779042874933286\n",
      "step: 468, acc: 0.875, loss: 0.341 (data_loss: 0.341, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.888, loss: 0.306 (data_loss: 0.306, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "validation, acc: 0.871, loss: 0.363\n"
     ]
    }
   ],
   "source": [
    "def load_mnist_dataset(dataset, path):\n",
    "    labels = sorted(os.listdir(os.path.join(path, dataset)))\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "def create_mnist_dataset(path):\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "\n",
    "    return X, y, X_test, y_test\n",
    "\n",
    "X, y, X_test, y_test = create_mnist_dataset('fashion_mnist_images')\n",
    "\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dropout(0.1))\n",
    "model.add(Layer_Dense(128, 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dropout(0.1))\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(loss=Loss_CategoricalCrossentropy(), optimizer=Optimizer_Adam(decay=1e-3), accuracy=Accuracy_Categorical())\n",
    "model.finalize()\n",
    "\n",
    "model.train(X, y, validation_data=(X_test, y_test), epochs=10, batch_size=128, print_every=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds-ai-env)",
   "language": "python",
   "name": "ds-ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
