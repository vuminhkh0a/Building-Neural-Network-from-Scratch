{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9e346f",
   "metadata": {},
   "source": [
    "# Neuron networks from scratch in Python\n",
    "References: http://103.203.175.90:81/fdScript/RootOfEBooks/E%20Book%20collection%20-%202024%20-%20G/CSE%20%20IT%20AIDS%20ML/Neural%20Network.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0666c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd975832",
   "metadata": {},
   "source": [
    "## Chapter 14: L1 & L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafd537",
   "metadata": {},
   "source": [
    "### 14.1 Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8688088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "   \n",
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_val = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_val / np.sum(exp_val, axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for i, (single_output, single_dvalue) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = np.reshape(single_output, (-1, 1))\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            self.dinputs[i] = np.dot(jacobian_matrix, single_dvalue)\n",
    "\n",
    "class Loss:\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        \n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights ** 2)\n",
    "\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        \n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases ** 2)\n",
    "        \n",
    "        return regularization_loss\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        return np.mean(self.forward(y_pred, y_true))\n",
    "\n",
    "class CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred[range(samples), y_true]\n",
    "        \n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_true * y_pred, axis=1)\n",
    "\n",
    "        return - np.log(correct_confidences)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = - y_true / dvalues / samples\n",
    "\n",
    "class Softmax_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Softmax()\n",
    "        self.loss = CategoricalCrossentropy()\n",
    "    \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs /= samples\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., eps=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate / (1. + self.decay * self.iterations)\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        layer.weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        layer.bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases ** 2\n",
    "\n",
    "        layer.weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        layer.bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights -= self.current_learning_rate * layer.weight_momentums_corrected / \\\n",
    "                        (np.sqrt(layer.weight_cache_corrected) + self.eps)\n",
    "        layer.biases -= self.current_learning_rate * layer.bias_momentums_corrected / \\\n",
    "                        (np.sqrt(layer.bias_cache_corrected) + self.eps)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d4fa14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, accuracy: 0.5966666666666667, loss: 0.9034391641616821, data_loss: 0.8798059225082397, reg_loss: 0.023633215576410294, lr: 0.019999020048017648\n",
      "epoch: 200, accuracy: 0.7466666666666667, loss: 0.699211835861206, data_loss: 0.6344795823097229, reg_loss: 0.06473222374916077, lr: 0.019998020196000596\n",
      "epoch: 300, accuracy: 0.8066666666666666, loss: 0.6128601431846619, data_loss: 0.5268653631210327, reg_loss: 0.08599480241537094, lr: 0.019997020443953854\n",
      "epoch: 400, accuracy: 0.83, loss: 0.5710640549659729, data_loss: 0.47757798433303833, reg_loss: 0.09348604828119278, lr: 0.01999602079186242\n",
      "epoch: 500, accuracy: 0.8466666666666667, loss: 0.5386819243431091, data_loss: 0.4438251852989197, reg_loss: 0.09485674649477005, lr: 0.019995021239711315\n",
      "epoch: 600, accuracy: 0.85, loss: 0.5164589881896973, data_loss: 0.4224472939968109, reg_loss: 0.09401170164346695, lr: 0.019994021787485543\n",
      "epoch: 700, accuracy: 0.8566666666666667, loss: 0.5000373721122742, data_loss: 0.4074549376964569, reg_loss: 0.09258242696523666, lr: 0.01999302243517013\n",
      "epoch: 800, accuracy: 0.8533333333333334, loss: 0.4872473478317261, data_loss: 0.39620763063430786, reg_loss: 0.09103970229625702, lr: 0.019992023182750084\n",
      "epoch: 900, accuracy: 0.8433333333333334, loss: 0.4747079610824585, data_loss: 0.38507962226867676, reg_loss: 0.08962833881378174, lr: 0.019991024030210438\n",
      "epoch: 1000, accuracy: 0.8633333333333333, loss: 0.46103811264038086, data_loss: 0.37246352434158325, reg_loss: 0.0885746031999588, lr: 0.01999002497753621\n",
      "epoch: 1100, accuracy: 0.86, loss: 0.45289790630340576, data_loss: 0.3654029071331024, reg_loss: 0.08749500662088394, lr: 0.019989026024712434\n",
      "epoch: 1200, accuracy: 0.87, loss: 0.44383785128593445, data_loss: 0.3574066758155823, reg_loss: 0.08643117547035217, lr: 0.019988027171724137\n",
      "epoch: 1300, accuracy: 0.8666666666666667, loss: 0.4359864294528961, data_loss: 0.35061585903167725, reg_loss: 0.08537057787179947, lr: 0.01998702841855636\n",
      "epoch: 1400, accuracy: 0.8733333333333333, loss: 0.4294261932373047, data_loss: 0.3450867831707001, reg_loss: 0.08433942496776581, lr: 0.01998602976519413\n",
      "epoch: 1500, accuracy: 0.8733333333333333, loss: 0.4222474694252014, data_loss: 0.33892765641212463, reg_loss: 0.08331982791423798, lr: 0.019985031211622494\n",
      "epoch: 1600, accuracy: 0.8733333333333333, loss: 0.41628146171569824, data_loss: 0.3337262272834778, reg_loss: 0.08255524188280106, lr: 0.0199840327578265\n",
      "epoch: 1700, accuracy: 0.8866666666666667, loss: 0.410147488117218, data_loss: 0.3285067677497864, reg_loss: 0.08164070546627045, lr: 0.01998303440379118\n",
      "epoch: 1800, accuracy: 0.88, loss: 0.4047560691833496, data_loss: 0.32398721575737, reg_loss: 0.08076885342597961, lr: 0.0199820361495016\n",
      "epoch: 1900, accuracy: 0.8833333333333333, loss: 0.39858362078666687, data_loss: 0.318673312664032, reg_loss: 0.07991030812263489, lr: 0.0199810379949428\n",
      "epoch: 2000, accuracy: 0.88, loss: 0.39092904329299927, data_loss: 0.31197503209114075, reg_loss: 0.07895401120185852, lr: 0.01998003994009984\n",
      "epoch: 2100, accuracy: 0.89, loss: 0.38605111837387085, data_loss: 0.30782195925712585, reg_loss: 0.0782291516661644, lr: 0.019979041984957778\n",
      "epoch: 2200, accuracy: 0.8933333333333333, loss: 0.38011041283607483, data_loss: 0.30254480242729187, reg_loss: 0.07756561785936356, lr: 0.01997804412950168\n",
      "epoch: 2300, accuracy: 0.8933333333333333, loss: 0.3779011368751526, data_loss: 0.30095845460891724, reg_loss: 0.07694267481565475, lr: 0.019977046373716598\n",
      "epoch: 2400, accuracy: 0.8966666666666666, loss: 0.3705659806728363, data_loss: 0.2942742109298706, reg_loss: 0.0762917771935463, lr: 0.019976048717587614\n",
      "epoch: 2500, accuracy: 0.8966666666666666, loss: 0.36814820766448975, data_loss: 0.29243701696395874, reg_loss: 0.0757111981511116, lr: 0.019975051161099787\n",
      "epoch: 2600, accuracy: 0.8933333333333333, loss: 0.36329248547554016, data_loss: 0.2882148027420044, reg_loss: 0.07507768273353577, lr: 0.019974053704238198\n",
      "epoch: 2700, accuracy: 0.9, loss: 0.36020076274871826, data_loss: 0.2857210338115692, reg_loss: 0.07447972893714905, lr: 0.019973056346987914\n",
      "epoch: 2800, accuracy: 0.8933333333333333, loss: 0.3553442060947418, data_loss: 0.28147515654563904, reg_loss: 0.07386905699968338, lr: 0.019972059089334023\n",
      "epoch: 2900, accuracy: 0.8966666666666666, loss: 0.3577079772949219, data_loss: 0.28439947962760925, reg_loss: 0.07330851256847382, lr: 0.0199710619312616\n",
      "epoch: 3000, accuracy: 0.8966666666666666, loss: 0.35006678104400635, data_loss: 0.277259886264801, reg_loss: 0.07280688732862473, lr: 0.019970064872755742\n",
      "epoch: 3100, accuracy: 0.9, loss: 0.3467246890068054, data_loss: 0.27434784173965454, reg_loss: 0.07237684726715088, lr: 0.01996906791380152\n",
      "epoch: 3200, accuracy: 0.9, loss: 0.34450095891952515, data_loss: 0.2726793885231018, reg_loss: 0.07182157039642334, lr: 0.019968071054384043\n",
      "epoch: 3300, accuracy: 0.9, loss: 0.34129658341407776, data_loss: 0.27002158761024475, reg_loss: 0.07127498835325241, lr: 0.01996707429448839\n",
      "epoch: 3400, accuracy: 0.9066666666666666, loss: 0.3390912413597107, data_loss: 0.2683744430541992, reg_loss: 0.07071679830551147, lr: 0.019966077634099667\n",
      "epoch: 3500, accuracy: 0.9, loss: 0.3371853232383728, data_loss: 0.2670563757419586, reg_loss: 0.07012893259525299, lr: 0.019965081073202967\n",
      "epoch: 3600, accuracy: 0.9033333333333333, loss: 0.33414795994758606, data_loss: 0.2645302712917328, reg_loss: 0.06961768120527267, lr: 0.019964084611783398\n",
      "epoch: 3700, accuracy: 0.9, loss: 0.33244508504867554, data_loss: 0.26336541771888733, reg_loss: 0.0690796822309494, lr: 0.019963088249826073\n",
      "epoch: 3800, accuracy: 0.9033333333333333, loss: 0.3301524221897125, data_loss: 0.26160258054733276, reg_loss: 0.06854983419179916, lr: 0.019962091987316084\n",
      "epoch: 3900, accuracy: 0.9033333333333333, loss: 0.33062994480133057, data_loss: 0.2625667452812195, reg_loss: 0.06806318461894989, lr: 0.01996109582423856\n",
      "epoch: 4000, accuracy: 0.9033333333333333, loss: 0.3271123170852661, data_loss: 0.2594553232192993, reg_loss: 0.0676569864153862, lr: 0.019960099760578602\n",
      "epoch: 4100, accuracy: 0.9066666666666666, loss: 0.3229799270629883, data_loss: 0.25564584136009216, reg_loss: 0.06733407080173492, lr: 0.019959103796321338\n",
      "epoch: 4200, accuracy: 0.9033333333333333, loss: 0.32081839442253113, data_loss: 0.25375881791114807, reg_loss: 0.06705958396196365, lr: 0.01995810793145188\n",
      "epoch: 4300, accuracy: 0.9133333333333333, loss: 0.3186267018318176, data_loss: 0.25184378027915955, reg_loss: 0.06678292900323868, lr: 0.019957112165955363\n",
      "epoch: 4400, accuracy: 0.9133333333333333, loss: 0.3176621198654175, data_loss: 0.25117170810699463, reg_loss: 0.06649041920900345, lr: 0.019956116499816903\n",
      "epoch: 4500, accuracy: 0.9133333333333333, loss: 0.3166601061820984, data_loss: 0.25042396783828735, reg_loss: 0.06623612344264984, lr: 0.019955120933021635\n",
      "epoch: 4600, accuracy: 0.9066666666666666, loss: 0.31456121802330017, data_loss: 0.24864478409290314, reg_loss: 0.06591642647981644, lr: 0.019954125465554688\n",
      "epoch: 4700, accuracy: 0.91, loss: 0.3124244213104248, data_loss: 0.2467649281024933, reg_loss: 0.06565947830677032, lr: 0.019953130097401205\n",
      "epoch: 4800, accuracy: 0.9066666666666666, loss: 0.3123528063297272, data_loss: 0.24704129993915558, reg_loss: 0.0653115063905716, lr: 0.019952134828546318\n",
      "epoch: 4900, accuracy: 0.9166666666666666, loss: 0.308332622051239, data_loss: 0.24330604076385498, reg_loss: 0.06502657383680344, lr: 0.019951139658975173\n",
      "epoch: 5000, accuracy: 0.9, loss: 0.3084624409675598, data_loss: 0.2437211126089096, reg_loss: 0.06474132835865021, lr: 0.019950144588672905\n",
      "epoch: 5100, accuracy: 0.9166666666666666, loss: 0.3062310516834259, data_loss: 0.24178120493888855, reg_loss: 0.06444983929395676, lr: 0.019949149617624676\n",
      "epoch: 5200, accuracy: 0.9166666666666666, loss: 0.3036889433860779, data_loss: 0.23954445123672485, reg_loss: 0.06414449214935303, lr: 0.019948154745815624\n",
      "epoch: 5300, accuracy: 0.9066666666666666, loss: 0.30307257175445557, data_loss: 0.23920659720897675, reg_loss: 0.06386596709489822, lr: 0.019947159973230915\n",
      "epoch: 5400, accuracy: 0.91, loss: 0.30730706453323364, data_loss: 0.24374842643737793, reg_loss: 0.06355862319469452, lr: 0.01994616529985569\n",
      "epoch: 5500, accuracy: 0.9166666666666666, loss: 0.3008785545825958, data_loss: 0.23766972124576569, reg_loss: 0.06320883333683014, lr: 0.019945170725675122\n",
      "epoch: 5600, accuracy: 0.9133333333333333, loss: 0.29918068647384644, data_loss: 0.23625218868255615, reg_loss: 0.06292851269245148, lr: 0.019944176250674364\n",
      "epoch: 5700, accuracy: 0.9133333333333333, loss: 0.29699885845184326, data_loss: 0.2343735694885254, reg_loss: 0.06262529641389847, lr: 0.019943181874838584\n",
      "epoch: 5800, accuracy: 0.9133333333333333, loss: 0.29602134227752686, data_loss: 0.23365747928619385, reg_loss: 0.06236385181546211, lr: 0.019942187598152954\n",
      "epoch: 5900, accuracy: 0.9133333333333333, loss: 0.30217933654785156, data_loss: 0.24015559256076813, reg_loss: 0.06202374026179314, lr: 0.019941193420602642\n",
      "epoch: 6000, accuracy: 0.92, loss: 0.2974330484867096, data_loss: 0.23559735715389252, reg_loss: 0.061835695058107376, lr: 0.019940199342172824\n",
      "epoch: 6100, accuracy: 0.9133333333333333, loss: 0.29560568928718567, data_loss: 0.23400436341762543, reg_loss: 0.06160132586956024, lr: 0.019939205362848673\n",
      "epoch: 6200, accuracy: 0.92, loss: 0.2954711318016052, data_loss: 0.23409515619277954, reg_loss: 0.061375975608825684, lr: 0.019938211482615376\n",
      "epoch: 6300, accuracy: 0.92, loss: 0.29156365990638733, data_loss: 0.23033156991004944, reg_loss: 0.06123209744691849, lr: 0.019937217701458107\n",
      "epoch: 6400, accuracy: 0.91, loss: 0.28948917984962463, data_loss: 0.22844231128692627, reg_loss: 0.06104686111211777, lr: 0.019936224019362063\n",
      "epoch: 6500, accuracy: 0.92, loss: 0.2874020040035248, data_loss: 0.22655609250068665, reg_loss: 0.06084590032696724, lr: 0.019935230436312422\n",
      "epoch: 6600, accuracy: 0.9233333333333333, loss: 0.28735488653182983, data_loss: 0.22673125565052032, reg_loss: 0.06062363460659981, lr: 0.019934236952294383\n",
      "epoch: 6700, accuracy: 0.9266666666666666, loss: 0.2869309186935425, data_loss: 0.2264656126499176, reg_loss: 0.060465309768915176, lr: 0.019933243567293136\n",
      "epoch: 6800, accuracy: 0.9233333333333333, loss: 0.2844786047935486, data_loss: 0.22413994371891022, reg_loss: 0.060338664799928665, lr: 0.019932250281293883\n",
      "epoch: 6900, accuracy: 0.92, loss: 0.28404152393341064, data_loss: 0.22364065051078796, reg_loss: 0.060400888323783875, lr: 0.019931257094281823\n",
      "epoch: 7000, accuracy: 0.9166666666666666, loss: 0.28247734904289246, data_loss: 0.2220616191625595, reg_loss: 0.06041572615504265, lr: 0.01993026400624216\n",
      "epoch: 7100, accuracy: 0.9166666666666666, loss: 0.2792009711265564, data_loss: 0.2188873291015625, reg_loss: 0.06031365692615509, lr: 0.019929271017160098\n",
      "epoch: 7200, accuracy: 0.9166666666666666, loss: 0.2825491428375244, data_loss: 0.22237573564052582, reg_loss: 0.06017342209815979, lr: 0.019928278127020853\n",
      "epoch: 7300, accuracy: 0.92, loss: 0.2783665359020233, data_loss: 0.21834498643875122, reg_loss: 0.06002155691385269, lr: 0.01992728533580963\n",
      "epoch: 7400, accuracy: 0.9133333333333333, loss: 0.2770610749721527, data_loss: 0.2171899676322937, reg_loss: 0.05987109988927841, lr: 0.019926292643511652\n",
      "epoch: 7500, accuracy: 0.9166666666666666, loss: 0.27747732400894165, data_loss: 0.21777606010437012, reg_loss: 0.05970127508044243, lr: 0.01992530005011213\n",
      "epoch: 7600, accuracy: 0.9133333333333333, loss: 0.2748652696609497, data_loss: 0.21533797681331635, reg_loss: 0.05952728912234306, lr: 0.019924307555596286\n",
      "epoch: 7700, accuracy: 0.92, loss: 0.27444353699684143, data_loss: 0.2151031792163849, reg_loss: 0.059340354055166245, lr: 0.019923315159949356\n",
      "epoch: 7800, accuracy: 0.92, loss: 0.2736416459083557, data_loss: 0.21448902785778046, reg_loss: 0.059152621775865555, lr: 0.019922322863156552\n",
      "epoch: 7900, accuracy: 0.9166666666666666, loss: 0.27199211716651917, data_loss: 0.21303048729896545, reg_loss: 0.058961622416973114, lr: 0.019921330665203112\n",
      "epoch: 8000, accuracy: 0.9133333333333333, loss: 0.27259987592697144, data_loss: 0.21381324529647827, reg_loss: 0.058786630630493164, lr: 0.019920338566074267\n",
      "epoch: 8100, accuracy: 0.9233333333333333, loss: 0.27390649914741516, data_loss: 0.21529608964920044, reg_loss: 0.05861041694879532, lr: 0.01991934656575526\n",
      "epoch: 8200, accuracy: 0.9233333333333333, loss: 0.2763773202896118, data_loss: 0.21793456375598907, reg_loss: 0.05844274163246155, lr: 0.019918354664231315\n",
      "epoch: 8300, accuracy: 0.9166666666666666, loss: 0.2695777714252472, data_loss: 0.2113291621208191, reg_loss: 0.0582486167550087, lr: 0.019917362861487688\n",
      "epoch: 8400, accuracy: 0.92, loss: 0.2682666778564453, data_loss: 0.21021409332752228, reg_loss: 0.05805256962776184, lr: 0.019916371157509615\n",
      "epoch: 8500, accuracy: 0.92, loss: 0.2679900527000427, data_loss: 0.2101200371980667, reg_loss: 0.057870008051395416, lr: 0.019915379552282352\n",
      "epoch: 8600, accuracy: 0.92, loss: 0.26740989089012146, data_loss: 0.20971305668354034, reg_loss: 0.05769682303071022, lr: 0.019914388045791143\n",
      "epoch: 8700, accuracy: 0.92, loss: 0.2656683921813965, data_loss: 0.20813044905662537, reg_loss: 0.05753793194890022, lr: 0.019913396638021247\n",
      "epoch: 8800, accuracy: 0.92, loss: 0.2657907009124756, data_loss: 0.20844276249408722, reg_loss: 0.057347945868968964, lr: 0.019912405328957914\n",
      "epoch: 8900, accuracy: 0.9233333333333333, loss: 0.26637715101242065, data_loss: 0.20920683443546295, reg_loss: 0.057170320302248, lr: 0.01991141411858641\n",
      "epoch: 9000, accuracy: 0.73, loss: 0.8706998825073242, data_loss: 0.8110666871070862, reg_loss: 0.05963318049907684, lr: 0.019910423006891994\n",
      "epoch: 9100, accuracy: 0.92, loss: 0.2665945887565613, data_loss: 0.20579290390014648, reg_loss: 0.06080169230699539, lr: 0.019909431993859934\n",
      "epoch: 9200, accuracy: 0.92, loss: 0.262755811214447, data_loss: 0.20210255682468414, reg_loss: 0.060653261840343475, lr: 0.01990844107947549\n",
      "epoch: 9300, accuracy: 0.92, loss: 0.2612631320953369, data_loss: 0.20065699517726898, reg_loss: 0.06060613691806793, lr: 0.01990745026372395\n",
      "epoch: 9400, accuracy: 0.92, loss: 0.26016780734062195, data_loss: 0.1995982676744461, reg_loss: 0.060569532215595245, lr: 0.019906459546590573\n",
      "epoch: 9500, accuracy: 0.92, loss: 0.2591165006160736, data_loss: 0.19856064021587372, reg_loss: 0.06055585294961929, lr: 0.01990546892806064\n",
      "epoch: 9600, accuracy: 0.92, loss: 0.2580728530883789, data_loss: 0.19751569628715515, reg_loss: 0.06055716425180435, lr: 0.019904478408119434\n",
      "epoch: 9700, accuracy: 0.92, loss: 0.25712090730667114, data_loss: 0.19655601680278778, reg_loss: 0.06056489422917366, lr: 0.019903487986752236\n",
      "epoch: 9800, accuracy: 0.92, loss: 0.2560170888900757, data_loss: 0.19544371962547302, reg_loss: 0.06057335436344147, lr: 0.01990249766394434\n",
      "epoch: 9900, accuracy: 0.92, loss: 0.254841685295105, data_loss: 0.19424013793468475, reg_loss: 0.06060153990983963, lr: 0.019901507439681016\n",
      "epoch: 10000, accuracy: 0.92, loss: 0.25382888317108154, data_loss: 0.19320444762706757, reg_loss: 0.06062443181872368, lr: 0.019900517313947576\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "activation1 = ReLU()\n",
    "dense2 = Dense(64, 3)\n",
    "loss_activation = Softmax_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.02, decay=5e-7)\n",
    "epochs = range(1, 10001)\n",
    "for epoch in epochs:\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, accuracy: {accuracy}, loss: {loss}, data_loss: {data_loss}, reg_loss: {regularization_loss}, lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e42926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.833, loss: 0.668\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds-ai-env)",
   "language": "python",
   "name": "ds-ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
